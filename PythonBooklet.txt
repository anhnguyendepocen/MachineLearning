*Machine Learning with Python and H2O*

by Spencer Aiello, Cliff Click, Hank Roark, and Ludi Rehak 

Edited by: Jessica Lanford

http://h2o.ai/resources/

July 2016: Third Edition

Contents

1 Introduction

2 What is H2O?  
2.1 Example Code  
2.2 Citation 

3 Installation 
3.1 Installation in Python 

4 Data Preparation  
4.1 Viewing Data  
4.2 Selection  
4.3 Missing Data  
4.4 Operations   
4.5 Merging   
4.6 Grouping  
4.7 Using Date and Time Data  
4.8 Categorical Variables  
4.9 Loading and Saving Data  

5 Machine Learning  
5.1 Modeling   
5.1.1 Supervised Learning  
5.1.2 Unsupervised Learning  
5.2 Running Models  
5.2.1 Gradient Boosting Models(GBM)  
5.2.2 Generalized Linear Models(GLM)  
5.2.3 K-means  
5.2.4 Principal Components Analysis (PCA)  
5.3 Grid Search  
5.4 Integration with scikit-learn  
5.4.1 Pipelines  
5.4.2 Randomized Grid Search  

6 References 39

# 1 Introduction

This documentation describes how to use H2O from Python. More information on H2O’s system and algorithms (as well as complete Python user documentation) is available at the H2O website at http://docs.h2o.ai.

H2O Python uses a REST API to connect to H2O. To use H2O in Python or launch H2O from Python, specify the IP address and port number of the H2O instance in the Python environment. Datasets are not directly transmitted through the REST API. Instead, commands (for example, importing a dataset at specified HDFS location) are sent either through the browser or the REST API to perform the specified task.

The dataset is then assigned an identifier that is used as a reference in commands to the web server. After one prepares the dataset for modeling by defining significant data and removing insignificant data, H2O is used to create a model representing the results of the data analysis. These models are assigned IDs that are used as references in commands.

Depending on the size of your data, H2O can run on your desktop or scale using multiple nodes with Hadoop, an EC2 cluster, or Spark. Hadoop is a scalable open-source file system that uses clusters for distributed storage and dataset processing. H2O nodes run as JVM invocations on Hadoop nodes. For performance reasons, we recommend that you do not run an H2O node on the same hardware as the Hadoop NameNode.

H2O helps Python users make the leap from single machine based processing to large-scale distributed environments. Hadoop lets H2O users scale their data processing capabilities based on their current needs. Using H2O, Python, and Hadoop, you can create a complete end-to-end data analysis solution.

This document describes the four steps of data analysis with H2O:

1. installing H2O

2. preparing your data for modeling

3. creating a model using simple but powerful machine learning algorithms 

4. scoring your models

# 2 What is H2O?

H2O is fast, scalable, open-source machine learning and deep learning for smarter applications. With H2O, enterprises like PayPal, Nielsen Catalina, Cisco, and others can use all their data without sampling to get accurate predictions faster. Advanced algorithms such as deep learning, boosting, and bagging ensembles are built-in to help application designers create smarter applications through elegant APIs. Some of our initial customers have built powerful domain-specific predictive engines for recommendations, customer churn, propensity to buy, dynamic pricing, and fraud detection for the insurance, healthcare, telecommunications, ad tech, retail, and payment systems industries.

Using in-memory compression, H2O handles billions of data rows in-memory, even with a small cluster. To make it easier for non-engineers to create complete analytic workflows, H2O’s platform includes interfaces for R, Python, Scala, Java, JSON, and CoffeeScript/JavaScript, as well as a built-in web interface, Flow. H2O is designed to run in standalone mode, on Hadoop, or within a Spark Cluster, and typically deploys within minutes.

H2O includes many common machine learning algorithms, such as generalized linear modeling (linear regression, logistic regression, etc.), Na ̈ıve Bayes, principal components analysis, k-means clustering, and others. H2O also implements best-in-class algorithms at scale, such as distributed random forest, gradient boosting, and deep learning. Customers can build thousands of models and compare the results to get the best predictions.

H2O is nurturing a grassroots movement of physicists, mathematicians, and computer scientists to herald the new wave of discovery with data science by collaborating closely with academic researchers and industrial data scientists. Stanford university giants Stephen Boyd, Trevor Hastie, Rob Tibshirani advise the H2O team on building scalable machine learning algorithms. With hundreds of meet-ups over the past three years, H2O has become a word-of-mouth phenomenon, growing amongst the data community by a hundred-fold, and is now used by 30,000+ users and is deployed using R, Python, Hadoop, and Spark in 2000+ corporations.

Try it out

- Download H2O directly at http://h2o.ai/download.

- Install H2O’s R package from CRAN at https://cran.r-project.
org/web/packages/h2o/.

- Install the Python package from PyPI at https://pypi.python. org/pypi/h2o/.

Join the community

- To learn about our meet-ups, training sessions, hackathons, and product updates, visit http://h2o.ai.

- Visit the open source community forum at https://groups.google.com/d/forum/h2ostream.

- Join the chat at https://gitter.im/h2oai/h2o-3.

## 2.1 Example Code

Python code for the examples in this document is located here:

https://github.com/h2oai/h2o-3/tree/master/h2o-docs/src/booklets/v2_2015/source/python

## 2.2 Citation

To cite this booklet, use the following:

Aiello, S., Cliff, C., Roark, H., Rehak, L., and Lanford, J. (Jul 2016). Machine Learning with Python and H2O. http://h2o.ai/resources/.

# 3 Installation

H2O requires Java; if you do not already have Java installed, install it from https://java.com/en/download/ before installing H2O.

The easiest way to directly install H2O is via a Python package.

(Note: The examples in this document were created with H2O version 3.8.3.2.)

## 3.1 Installation in Python

To load a recent H2O package from PyPI, run:

pip install h2o

To download the latest stable H2O-3 build from the H2O download page: 

1. Go to http://h2o.ai/download.

2. Choose the latest stable H2O-3 build.

3. Click the “Install in Python” tab.

4. Copy and paste the commands into your Python session. 

After H2O is installed, verify the installation:

import h2o

# Start H2O on your local machine
h2o.init()

# Get help
help(h2o.estimators.glm.H2OGeneralizedLinearEstimator)
help(h2o.estimators.gbm.H2OGradientBoostingEstimator)

# Show a demo
h2o.demo("glm")
h2o.demo("gbm")

# 4 Data Preparation

The next sections of the booklet demonstrate the Python interface using examples, which include short snippets of code and the resulting output.

In H2O, these operations all occur distributed and in parallel and can be used on very large datasets. More information about the Python interface to H2O can be found at docs.h2o.ai.

Typically, we import and start H2O on the same machine as the running Python process:

import h2o 
h2o.init()

To connect to an established H2O cluster (in a multi-node Hadoop environment, for example):

h2o.init(ip = "123.45.67.89", port = 54321)

To create an H2OFrame object from a Python tuple:

df = h2o.H2OFrame(zip(*((1, 2, 3),
                   (’a’, ’b’, ’c’),
                   (0.1, 0.2, 0.3))))
df

To create an H2OFrame object from a Python list:

df = h2o.H2OFrame(zip(*[[1, 2, 3],
                   [’a’, ’b’, ’c’],
                   [0.1, 0.2, 0.3]]))
df

To create an H2OFrame object from collections.OrderedDict or a Python dict:

df = h2o.H2OFrame({’A’: [1, 2, 3],
                   ’B’: [’a’, ’b’, ’c’],
                   ’C’: [0.1, 0.2, 0.3]})
df

To create an H2OFrame object from a Python dict and specify the column types:

df2 = h2o.H2OFrame.from_python({
    ’A’: [1, 2, 3],
    ’B’: [’a’, ’a’, ’b’],
    ’C’: [’hello’, ’all’, ’world’],
    ’D’: [’12MAR2015:11:00:00’, ’13MAR2015:12:00:00’, ’14MAR2015:13:00:00’]},
    column_types = [’numeric’, ’enum’, ’string’, ’time’])
df2

To display the column types:

df2.types

## 4.1 Viewing Data

To display the top and bottom of an H2OFrame:

import numpy as np

df = h2o.H2OFrame.from_python(
    np.random.randn(4,100).tolist(), 
    column_names = list(’ABCD’))
df.head()
df.tail(5)

To display the column names:

df.columns

To display compression information, distribution (in multi-machine clusters), and summary statistics of your data:

df.describe()

## 4.2 Selection

To select a single column by name, resulting in an H2OFrame:

df[’A’]

To select a single column by index, resulting in an H2OFrame:

df[1]

To select multiple columns by name, resulting in an H2OFrame:

df[[’B’, ’C’]]

To select multiple columns by index, resulting in an H2OFrame:

df[0:2]

To select multiple rows by slicing, resulting in an H2OFrame:

Note By default, H2OFrame selection is for columns, so to slice by rows and get all columns, be explicit about selecting all columns:

df[2:7, :]

To select rows based on specific criteria, use Boolean masking:

df2[ df2["B"] == "a", :]

## 4.3 Missing Data

The H2O parser can handle many different representations of missing data types, including ‘’ (blank), ‘NA’, and None (Python). They are all displayed as NaN in Python.

To create an H2OFrame from Python with missing elements:

df3 = h2o.H2OFrame.from_python(
    {’A’: [1, 2, 3,None,’’],
     ’B’: [’a’, ’a’, ’b’, ’NA’, ’NA’],
     ’C’: [’hello’, ’all’, ’world’, None, None],
     ’D’: [’12MAR2015:11:00:00’,None,
           ’13MAR2015:12:00:00’,None,
           ’14MAR2015:13:00:00’]},
    column_types=[’numeric’, ’enum’, ’string’, ’time’])

df3

To determine which rows are missing data for a given column (‘1’ indicates missing):

df3["A"].isna()

To change all missing values in a column to a different value:

df3

To determine the locations of all missing data in an H2OFrame:

df3.isna()

## 4.4 Operations

When performing a descriptive statistic on an entire H2OFrame, missing data is generally excluded and the operation is only performed on the columns of the appropriate data type:

df3 = h2o.H2OFrame.from_python(
    {’A’: [1, 2, 3,None,’’],
     ’B’: [’a’, ’a’, ’b’, ’NA’, ’NA’],
     ’C’: [’hello’, ’all’, ’world’, None, None],
     ’D’: [’12MAR2015:11:00:00’, None,
           ’13MAR2015:12:00:00’, None,
           ’14MAR2015:13:00:00’]},
    column_types = [’numeric’, ’enum’, ’string’, ’time’])

df4.mean(na_rm = True)

When performing a descriptive statistic on a single column of an H2OFrame, missing data is generally not excluded:

df4["A"].mean()
df4["A"].mean(na_rm = True)

In both examples, a native Python object is returned (list and float respectively in these examples).

When applying functions to each column of the data, an H2OFrame containing the means of each column is returned:

df5 = h2o.H2OFrame.from_python(
    np.random.randn(4,100).tolist(),
    column_names = list(’ABCD’))

df5.apply(lambda x: x.mean(na_rm = True))

When applying functions to each row of the data, an H2OFrame containing the sum of all columns is returned:

df5.apply(lambda row: sum(row), axis = 1)

H2O provides many methods for histogramming and discretizing data. Here is an example using the hist method on a single data frame:

df6 = h2o.H2OFrame(np.random.randint(0, 7, size = 100).tolist())
df6.hist(plot=False) 

H2O includes a set of string processing methods in the H2OFrame class that make it easy to operate on each element in an H2OFrame.

To determine the number of times a string is contained in each element:

df7 = h2o.H2OFrame.from_python(
    [’Hello’, ’World’, ’Welcome’, ’To’, ’H2O’, ’World’])
df7
df7.countmatches(’l’)

To replace the first occurrence of ‘l’ (lower case letter) with ‘x’ and return a new H2OFrame:

df7.sub(’l’,’x’)

For global substitution, use gsub. Both sub and gsub support regular expressions. To split strings based on a regular expression:

df7.strsplit(’(l)+’)

## 4.5 Merging

To combine two H2OFrames together by appending one as rows and return a new H2OFrame:

df8 = h2o.H2OFrame.from_python(np.random.randn(100, 4).tolist(),
    column_names=list(’ABCD’))

df9 = h2o.H2OFrame.from_python(
    np.random.randn(100, 4).tolist(), 
    column_names=list(’ABCD’))

df8.rbind(df9)

For successful row binding, the column names and column types between the two H2OFrames must match.

H2O also supports merging two frames together by matching column names:

df10 = h2o.H2OFrame.from_python({
    ’A’: [’Hello’, ’World’, ’Welcome’, ’To’, ’H2O’, ’World’],
    ’n’: [0,1,2,3,4,5]})

df11 = h2o.H2OFrame.from_python(
    np.random.randint(0, 10, size = 100).tolist9), 
    column_names = [’n’])

df11.merge(df10)

## 4.6 Grouping

”Grouping” refers to the following process:

- splitting the data into groups based on some criteria

- applying a function to each group independently

- combining the results into an H2OFrame

To group and then apply a function to the results:

df12 = h2o.H2OFrame({
    ’A’: [’foo’, ’bar’, ’foo’, ’bar’, ’foo’, ’bar’, ’foo’, ’foo’],
    ’B’: [’one’, ’one’, ’two’, ’three’, ’two’, ’two’, ’one’, ’three’],
    ’C’: np.random.randn(8),
    ’D’: np.random.randn(8)})

df12

df12.group_by(’A’).sum().frame

To group by multiple columns and then apply a function:

df13 = df12.group_by([’A’,’B’]).sum().frame
df13

To join the results into the original H2OFrame:

df12.merge(df13)

## 4.7 Using Date and Time Data

H2O has powerful features for ingesting and feature engineering using time data. Internally, H2O stores time information as an integer of the number of milliseconds since the epoch.

To ingest time data natively, use one of the supported time input formats:

df14 = h2o.H2OFrame.from_python({
    ’D’: [’18OCT2015:11:00:00’, ’19OCT2015:12:00:00’, ’20OCT2015:13:00:00’]},
    column_types = [’time’])
df14.types

To display the day of the month:

df14[’D’].day()

To display the day of the week:

df14[’D’].dayOfWeek()

## 4.8 Categorical Variables

H2O handles categorical (also known as enumerated or factor) values in an H2OFrame. This is significant because categorical columns have specific treatments in each of the machine learning algorithms.

Using ‘df12’ from above, H2O imports columns A and B as categorical/enumerated/factor types:

df12.types

To determine if any column is a categorical/enumerated/factor type:

df12.anyfactor()

To view the categorical levels in a single column:

df12["A"].levels()

To create categorical interaction features:

df12.interaction([’A’,’B’], pairwise = False, max_factors = 3,
    min_occurrence = 1)

To retain the most common categories and set the remaining categories to a common ‘Other’ category and create an interaction of a categorical column with itself:

bb_df = df12.interaction([’B’, ’B’], pairwise = False,
    max_factors = 2, min_occurrence = 1)
bb_df

These can then be added as a new column on the original dataframe:

df15 = df12.cbind(bb_df)
df15

## 4.9 Loading and Saving Data

In addition to loading data from Python objects, H2O can load data directly from:

- disk

- network file systems (NFS, S3)

- distributed file systems (HDFS)

- HTTP addresses

H2O currently supports the following file types:

- CSV (delimited) files 

- ORC

- SVMLite

- ARFF 

- XLS 

- XLST

To load data from the same machine running H2O:

df = h2o.upload_file("/pathToFile/fileName")

To load data from the machine running Python to the machine running H2O:

df = h2o.import_file("/pathToFile/fileName")

To save an H2OFrame on the machine running H2O:

h2o.export_file(df,"/pathToFile/fileName")

To save an H2OFrame on the machine running Python:

h2o.download_csv(df,"/pathToFile/fileName")

# 5 Machine Learning

The following sections describe some common model types and features.

## 5.1 Modeling

The following section describes the features and functions of some common models available in H2O. For more information about running these models in Python using H2O, refer to the documentation on the H2O.ai website or to the booklets on specific models.

H2O supports the following models:

- Deep Learning

- Naive Bayes

- Principal Components Analysis (PCA)

- K-means

- Generalized Linear Models (GLM)

- Gradient Boosted Regression (GBM)

- Distributed Random Forest (DRF)

The list is growing quickly, so check www.h2o.ai to see the latest additions.

### 5.1.1 Supervised Learning

Generalized Linear Models (GLM): Provides flexible generalization of ordinary linear regression for response variables with error distribution models other than a Gaussian (normal) distribution. GLM unifies various other statistical models, including Poisson, linear, logistic, and others when using l1 and l2 regularization.

Distributed Random Forest: Averages multiple decision trees, each created on different random samples of rows and columns. It is easy to use, non-linear, and provides feedback on the importance of each predictor in the model, making it one of the most robust algorithms for noisy data.

Gradient Boosting (GBM): Produces a prediction model in the form of an ensemble of weak prediction models. It builds the model in a stage-wise fashion and is generalized by allowing an arbitrary differentiable loss function. It is one of the most powerful methods available today.

Deep Learning: Models high-level abstractions in data by using non-linear transformations in a layer-by-layer method. Deep learning is an example of supervised learning, which can use unlabeled data that other algorithms cannot.

Naive Bayes: Generates a probabilistic classifier that assumes the value of a particular feature is unrelated to the presence or absence of any other feature, given the class variable. It is often used in text categorization.

### 5.1.2 Unsupervised Learning

K-Means: Reveals groups or clusters of data points for segmentation. It clusters observations into k-number of points with the nearest mean.

Principal Component Analysis (PCA): The algorithm is carried out on a set of possibly collinear features and performs a transformation to produce a new set of uncorrelated features.

Anomaly Detection: Identifies the outliers in your data by invoking the deep learning autoencoder, a powerful pattern recognition model.

## 5.2 Running Models

This section describes how to run the following model types:

- Gradient Boosted Models (GBM)

- Generalized Linear Models (GLM)

- K-means

- Principal Components Analysis (PCA) as well as how to generate predictions.

### 5.2.1 Gradient Boosting Models (GBM)

To generate gradient boosting models for creating forward-learning ensembles, use H2OGradientBoostingEstimator.

The construction of the estimator defines the parameters of the estimator and the call to H2OGradientBoostingEstimator.train trains the estimator on the specified data. This pattern is common for each of the H2O algorithms.

import h2o 

h2o.init() 

from h2o.estimators.gbm import H2OGradientBoostingEstimator

iris_data_path = h2o.system_file("iris.csv") # load demonstration

iris_df = h2o.import_file(path = iris_data_path) 

iris_df.describe()

gbm_regressor = H2OGradientBoostingEstimator(
    distribution = "gaussian",
    ntrees = 10, 
    max_depth = 3, 
    min_rows = 2, 
    learn_rate = "0.2")

gbm_regressor.train(
    x = range(1, iris_df.ncol), 
    y = 0, 
    training_frame = iris_df)

gbm_regressor

To generate a classification model that uses labels, use distribution = "multinomial":

gbm_classifier = H2OGradientBoostingEstimator(
    distribution = "multinomial", 
    ntrees = 10, 
    max_depth = 3, 
    min_rows = 2, 
    learn_rate = "0.2")

gbm_classifier.train(
    x = range(0, iris_df.ncol - 1), 
    y = iris_df.ncol - 1,
    training_frame = iris_df) 

gbm_classifier

### 5.2.2 Generalized Linear Models (GLM)

Generalized linear models (GLM) are some of the most commonly-used models for many types of data analysis use cases. While some data can be analyzed using linear models, linear models may not be as accurate if the variables are more complex. For example, if the dependent variable has a non-continuous distribution or if the effect of the predictors is not linear, generalized linear models will produce more accurate results than linear models.

Generalized Linear Models (GLM) estimate regression models for outcomes following exponential distributions in general. In addition to the Gaussian (i.e. normal) distribution, these include Poisson, binomial, gamma and Tweedie distributions. Each serves a different purpose and, depending on distribution and link function choice, it can be used either for prediction or classification.

H2O’s GLM algorithm fits the generalized linear model with elastic net penalties. The model fitting computation is distributed, extremely fast,and scales extremely well for models with a limited number (∼low thousands) of predictors with non-zero coefficients.

The algorithm can compute models for a single value of a penalty argument or the full regularization path, similar to glmnet. It can compute Gaussian (linear), logistic, Poisson, and gamma regression models. To generate a generalized linear model for developing linear models for exponential distributions, use H2OGeneralizedLinearEstimator. You can apply regularization to the model by adjusting the lambda and alpha parameters.

from h2o.estimators.glm import H2OGeneralizedLinearEstimator 

prostate_data_path = h2o.system_file("prostate.csv")

prostate_df = h2o.import_file(path=prostate_data_path) 

prostate_df["RACE"] = prostate_df["RACE"].asfactor()

prostate_df.describe()

glm_classifier = H2OGeneralizedLinearEstimator(
    family = "binomial",
    nfolds = 10, 
    alpha = 0.5)

glm_classifier.train(
    x = ["AGE", "RACE", "PSA", "DCAPS"],
    y = "CAPSULE",
    training_frame = prostate_df) 

glm_classifier

### 5.2.3 K-means

To generate a K-means model for data characterization, use h2o.kmeans(). This algorithm does not require a dependent variable.

from h2o.estimators.kmeans import H2OKMeansEstimator

cluster_estimator = H2OKMeansEstimator(k = 3)

cluster_estimator.train(x = [0, 1, 2, 3], training_frame = iris_df) 

### 5.2.4 Principal Components Analysis (PCA)

To map a set of variables onto a subspace using linear transformations, use h2o.transforms.decomposition.H2OPCA. This is the first step in Principal Components Regression.

from h2o.transforms.decomposition import H2OPCA

pca_decomp = H2OPCA(k = 2, transform = "NONE", pca_method = "Power") 

pca_decomp.train(x = range(0,4), training_frame = iris_df)

pca_decomp

pred = pca_decomp.predict(iris_df) 

pred.head() # Projection results

## 5.3 Grid Search

H2O supports grid search across hyperparameters:

ntrees_opt = [5, 10, 15] 

max_depth_opt = [2, 3, 4] 

learn_rate_opt = [0.1, 0.2]

hyper_parameters = {
    "ntrees": ntrees_opt, 
    "max_depth": max_depth_opt,
    "learn_rate":learn_rate_opt}

from h2o.grid.grid_search import H2OGridSearch

gs = H2OGridSearch(H2OGradientBoostingEstimator(
    distribution = "multinomial"), 
    hyper_params = hyper_parameters)

gs.train(
    x = range(0, iris_df.ncol - 1), 
    y = iris_df.ncol - 1, 
    training_frame = iris_df, 
    nfold = 10)

print gs.sort_by(’logloss’, increasing = True)

## 5.4 Integration with scikit-learn

The H2O Python client can be used within scikit-learn pipelines and cross-validation searches. This extends the capabilities of both H2O and scikit-learn.

### 5.4.1 Pipelines

To create a scikit-learn style pipeline using H2O transformers and estimators:

from h2o.transforms.preprocessing import H2OScaler 
from sklearn.pipeline import Pipeline

# Turn off h2o progress bars
h2o.__PROGRESS_BAR__ = False

h2o.no_progress()

# build transformation pipeline using sklearn’s Pipeline and H2O transforms
pipeline = Pipeline([
    ("standardize", H2OScaler()),
    ("pca", H2OPCA(k = 2)),
    ("gbm", H2OGradientBoostingEstimator(distribution = "multinomial"))])

pipeline.fit(iris_df[:4], iris_df[4])

### 5.4.2 Randomized Grid Search

To create a scikit-learn style hyperparameter grid search using k-fold cross validation:

from sklearn.grid_search import RandomizedSearchCV
from h2o.cross_validation import H2OKFold
from h2o.model.regression import h2o_r2_score
from sklearn.metrics.scorer import make_scorer
from sklearn.metrics.scorer import make_scorer

params = {
    "standardize__center": [True, False], # Parameters to test
    "standardize__scale": [True, False],
    "pca__k": [2, 3],
    "gbm__ntrees": [10, 20],
    "gbm__max_depth": [1, 2, 3],
    "gbm__learn_rate": [0.1, 0.2]}

custom_cv = H2OKFold(iris_df, n_folds = 5, seed = 42)

pipeline = Pipeline([
    ("standardize", H2OScaler()),
    ("pca", H2OPCA(k = 2)),
    ("gbm", H2OGradientBoostingEstimator(distribution = "gaussian"))])

random_search = RandomizedSearchCV(
    pipeline, 
    params,
    n_iter = 5,
    scoring = make_scorer(h2o_r2_score),
    cv = custom_cv,
    random_state = 42,
    n_jobs = 1)

random_search.fit(iris_df[1:], iris_df[0])

print random_search.best_estimator_

# 6 References

H2O.ai Team. H2O website, 2016. URL http://h2o.ai

H2O.ai Team. H2O documentation, 2016. URL http://docs.h2o.ai

H2O.ai Team. H2O Python Documentation, 2015. URL http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Pydoc.html

H2O.ai Team. H2O GitHub Repository, 2016. URL https://github. com/h2oai

H2O.ai Team. H2O Datasets, 2016. URL http://data.h2o.ai H2O.ai Team. H2O JIRA, 2016. URL https://jira.h2o.ai

H2O.ai Team. H2Ostream, 2016. URL https://groups.google.com/d/forum/h2ostream
